{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Tweet Prediction .ipynb","provenance":[{"file_id":"101AYQv-e0a9kN4qW6ogbcwlMa7CME4ER","timestamp":1618763156534},{"file_id":"12DMcGRBq0za4pQ5Fkr6krA4fu5eFgKFj","timestamp":1618758456517},{"file_id":"1lAF0akDHn1psLqwicJokyXH_Gt0ivEk-","timestamp":1618738822595}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W8x_aYfh7xVL","executionInfo":{"status":"ok","timestamp":1618848365995,"user_tz":-480,"elapsed":19416,"user":{"displayName":"Jun Yi Chan","photoUrl":"","userId":"11999584248492493983"}},"outputId":"709c508b-d994-4a36-f684-329c7116161d"},"source":["\"\"\" Prepare Notebook for Google Colab \"\"\"\n","# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Vm6suMCaYH7M","executionInfo":{"status":"ok","timestamp":1618849338864,"user_tz":-480,"elapsed":3525,"user":{"displayName":"Jun Yi Chan","photoUrl":"","userId":"11999584248492493983"}},"outputId":"5b28f426-1824-47e8-8cd0-f6077470b229"},"source":["##### Google Colab #####\n","# Specify directory of course materials in Google Drive\n","module_dir = '/content/drive/My Drive/Colab Notebooks/COMP3359/Project/'\n","\n","# Install necessary packages\n","!pip install pyspellchecker \n","\n","import sys\n","import pandas as pd\n","import string\n","import os\n","import numpy as np\n","import nltk\n","import math\n","import tqdm\n","import tensorflow as tf\n","import re as regex\n","\n","sys.path.append(module_dir)\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","from nltk.tokenize import casual_tokenize\n","from nltk.corpus import stopwords\n","from nltk.stem import PorterStemmer\n","porter = PorterStemmer()\n","from spellchecker import SpellChecker\n","spell = SpellChecker(language=None, case_sensitive=True)\n","from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, GlobalAveragePooling1D, Dropout\n","from tensorflow.keras import Sequential\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from tensorflow import keras"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: pyspellchecker in /usr/local/lib/python3.7/dist-packages (0.6.2)\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Q_XhVXwVZJzu","executionInfo":{"status":"ok","timestamp":1618848373276,"user_tz":-480,"elapsed":1437,"user":{"displayName":"Jun Yi Chan","photoUrl":"","userId":"11999584248492493983"}}},"source":["##### Initialize Classes #####\n","class TwitterData_Initialize():\n","    data = []\n","    processed_data = []\n","    \n","    def initialize(self, twitter_data):\n","        self.data = twitter_data\n","        self.processed_data = self.data\n","\n","class TwitterCleanuper:\n","    def iterate(self):\n","        for cleanup_method in [self.remove_urls,\n","                               self.remove_usernames,\n","                               self.remove_numbers,\n","                               self.remove_special_chars,\n","                               self.remove_duplicate_characters,\n","                               self.correct_spelling,\n","                               self.remove_stopwords]:\n","            yield cleanup_method\n","\n","    def remove_by_regex(tweets, regexp):\n","        tweets.loc[:, \"tweet\"].replace(regexp, \"\", inplace=True)\n","        return tweets\n","\n","    def remove_urls(self, tweets):\n","        return TwitterCleanuper.remove_by_regex(tweets, regex.compile(r\"http.?://[^\\s]+[\\s]?\"))\n","\n","    def remove_special_chars(self, tweets):  # it unrolls the hashtags to normal words\n","        for remove in map(lambda r: regex.compile(regex.escape(r)), [\",\", \":\", \"\\\"\", \"=\", \"&\", \";\", \"%\", \"$\",\n","                                                                     \"@\", \"%\", \"^\", \"*\", \"(\", \")\", \"{\", \"}\",\n","                                                                     \"[\", \"]\", \"|\", \"/\", \"\\\\\", \">\", \"<\", \"-\",\n","                                                                     \"!\", \"?\", \".\", \"'\",\n","                                                                     \"--\", \"---\", \"#\"]):\n","            tweets.loc[:, \"tweet\"].replace(remove, \"\", inplace=True)\n","        return tweets\n","\n","    def remove_usernames(self, tweets):\n","        return TwitterCleanuper.remove_by_regex(tweets, regex.compile(r\"@[^\\s]+[\\s]?\"))\n","\n","    def remove_numbers(self, tweets):\n","        tweets[\"tweet\"] = tweets[\"tweet\"].apply(lambda x: regex.sub(\"\\S*\\d\\S*\", \"\", x).strip()) # to remove word entirely\n","        return tweets\n","      \n","    def remove_duplicate_characters(self, tweets):\n","        tweets[\"tweet\"] = tweets[\"tweet\"].apply(lambda x: regex.sub(r'([a-zA-Z])\\1\\1+', r'\\1\\1', x))\n","        return tweets\n","\n","    def correct_spelling(self, tweets):\n","        tweets[\"tweet\"] = tweets[\"tweet\"].apply(lambda x: spell.correction(x).lower())\n","        return tweets\n","\n","    def remove_stopwords(self, tweets):\n","        pattern = regex.compile(r'\\b(' + r'|'.join(stopwords.words('english')) + r')\\b\\s*')\n","        tweets[\"tweet\"] = tweets[\"tweet\"].apply(lambda x: pattern.sub('', x))\n","        return tweets\n","\n","\n","class TwitterData_Cleansing(TwitterData_Initialize):\n","    def __init__(self, previous):\n","        self.processed_data = previous.processed_data\n","        \n","    def cleanup(self, cleanuper):\n","        t = self.processed_data\n","        for cleanup_method in cleanuper.iterate():\n","            t = cleanup_method(t)\n","        self.processed_data = t\n","\n","class TwitterData_TokenStem(TwitterData_Cleansing):\n","    def __init__(self, previous):\n","        self.processed_data = previous.processed_data\n","        \n","    def stem(self, stemmer = porter ):\n","        def stem_and_join(tweets):\n","            tweets[\"tweet\"] = list(map(lambda str: stemmer.stem(str.lower()), tweets['tweet']))\n","            return tweets\n","        self.processed_data = self.processed_data.apply(stem_and_join, axis=1)\n","\n","    def tokenize(self, tokenizer = casual_tokenize):\n","        t = self.processed_data\n","        t['tweet'] = t['tweet'].apply(tokenizer).tolist()\n","        # t['tokenized_tweet'] = t['tweet']\n","        self.processed_data = t\n","\n","class TwitterData_Padding(TwitterData_TokenStem):\n","    def __init__(self, previous):\n","        self.processed_data = previous.processed_data\n"," \n","    def padding(self, tokenizer = Tokenizer()):\n","        t = self.processed_data\n","        self.tokenizer = tokenizer\n","        t['token'] = t['tweet']\n","        pd.Series(t['token'])\n","        tokenizer.fit_on_texts(t['token'])\n","        t['token'] = tokenizer.texts_to_sequences(t['token'])  \n","        self.processed_data = t"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"vUBfFrv1mHuo","executionInfo":{"status":"ok","timestamp":1618848373277,"user_tz":-480,"elapsed":617,"user":{"displayName":"Jun Yi Chan","photoUrl":"","userId":"11999584248492493983"}}},"source":["# Replace the emoticons\n","import re as regex\n","\n","positive_emoticons = [\":)\", \":-)\", \":p\", \":-p\", \":P\", \":-P\", \";D\", \";-D\", \":D\",\":-D\", \":]\", \":-]\", \";)\", \";-)\", \";p\", \";-p\", \";P\", \";-P\", \";D\", \";-D\", \";]\", \";-]\", \"=)\", \"=-)\", \"<3\"]\n","negative_emoticons = [\":o\", \"D;\", \"D:\", \"D-:\", \":-o\", \":O\", \":-O\", \":(\", \":-(\", \":c\", \":-c\", \":C\", \":-C\", \":[\", \":-[\", \":/\", \":-/\", \":\\\\\", \":-\\\\\", \":n\", \":-n\", \":u\", \":-u\", \"=(\", \"=-(\", \":$\", \":-$\"]\n","\n","def make_emoticon_pattern(emoticons):\n","    pattern = \"|\".join(map(regex.escape, emoticons))\n","    pattern = \"(?<=\\s)(\" + pattern + \")(?=\\s)\"\n","    return pattern\n","\n","def replace_emoticons(tweets, pattern, tag):\n","    tweets[\"tweet\"]= tweets[\"tweet\"].apply(lambda x: regex.sub(pattern, tag, \" \" + x + \" \"))\n","    return tweets\n"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"NFEr6WFvERIq","executionInfo":{"status":"ok","timestamp":1618848375149,"user_tz":-480,"elapsed":1057,"user":{"displayName":"Jun Yi Chan","photoUrl":"","userId":"11999584248492493983"}}},"source":["def truncating(dataframe):\n","  count = []\n","  for i in range(len(dataframe['token'])):\n","    count.append(len(dataframe['token'][i]))\n","  dataframe['count'] = count\n","  dataframe = dataframe[(dataframe[['count']] != 0).all(axis=1)]\n","  mean_count = dataframe['count'].sum() / len(dataframe.index)\n","  int_mean = math.ceil(mean_count)\n","  postpad_maxlen_posttrunc = pad_sequences(dataframe['token'], padding = 'post', maxlen = int_mean, truncating = 'post')\n","  dataframe = dataframe.drop(['token','count'], axis=1)\n","  return dataframe, postpad_maxlen_posttrunc"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"QEMMzY8AB17x","executionInfo":{"status":"ok","timestamp":1618848375524,"user_tz":-480,"elapsed":616,"user":{"displayName":"Jun Yi Chan","photoUrl":"","userId":"11999584248492493983"}}},"source":["def Preprocess(dataframe):\n","  data = TwitterData_Initialize()\n","  data.initialize(dataframe)\n","  data.processed_data = replace_emoticons(data.processed_data, make_emoticon_pattern(positive_emoticons), 'positive_emoticons')\n","  data.processed_data = replace_emoticons(data.processed_data, make_emoticon_pattern(negative_emoticons), 'negative_emoticons')\n","  data = TwitterData_Cleansing(data)\n","  data.cleanup(TwitterCleanuper())\n","  data = TwitterData_TokenStem(data)\n","  data.tokenize()\n","  data.stem()\n","  data = TwitterData_Padding(data)\n","  data.padding()\n","  word_index = data.tokenizer.word_index\n","  truncated_data, postpad_maxlen_posttrunc = truncating(data.processed_data)\n","  return truncated_data, postpad_maxlen_posttrunc, word_index"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"tLpbLf4aC_uy","executionInfo":{"status":"ok","timestamp":1618848379531,"user_tz":-480,"elapsed":1056,"user":{"displayName":"Jun Yi Chan","photoUrl":"","userId":"11999584248492493983"}}},"source":["def construct_embedding_matrix(glove_file, word_index, EMBEDDING_VECTOR_LENGTH = 100 ):\n","    embedding_dict = {}\n","    with open(glove_file,'r', encoding='utf8') as f:\n","        for line in f:\n","            values=line.split()\n","            # get the word\n","            word=values[0]\n","            if word in word_index.keys():\n","                # get the vector\n","                vector = np.asarray(values[1:], 'float32')\n","                embedding_dict[word] = vector\n","    ###  oov words (out of vacabulary words) will be mapped to 0 vectors\n","\n","    num_words=len(word_index)+1\n","    #initialize it to 0\n","    embedding_matrix=np.zeros((num_words, EMBEDDING_VECTOR_LENGTH))\n","\n","    for word,i in tqdm.tqdm(word_index.items()):\n","        if i < num_words:\n","            vect=embedding_dict.get(word, [])\n","            if len(vect)>0:\n","                embedding_matrix[i] = vect[:EMBEDDING_VECTOR_LENGTH]\n","    return embedding_matrix\n","\n","def Prediction(embedding_matrix, postpad_maxlen_posttrunc, weight_file):\n","  x_test = postpad_maxlen_posttrunc\n","  VOCAB_SIZE = embedding_matrix.shape[0]\n","  EMBEDDING_DIM = 100\n","  model2 = Sequential(\n","    [\n","      Embedding(VOCAB_SIZE, EMBEDDING_DIM, weights = [embedding_matrix], trainable = False),\n","      Bidirectional(LSTM(EMBEDDING_DIM, dropout=0.1, return_sequences=True)),\n","      Bidirectional(LSTM(EMBEDDING_DIM)),\n","      Dense(24, activation='relu'), \n","      Dense(24, activation='relu'),\n","      Dense(1, activation='sigmoid')\n","    ]\n","  )\n","  adam = tf.keras.optimizers.Adam(learning_rate = 0.01)\n","  model2.compile(loss = 'binary_crossentropy', optimizer = adam, metrics = ['accuracy'])\n","  model2.load_weights(weight_file)\n","  prediction = model2.predict(x_test)\n","  return np.round(prediction)\n","\n","def modelpred(dataframe,postpad_maxlen_posttrunc,path):\n","  model = keras.models.load_model(path)\n","  prediction = model2.predict(postpad_maxlen_posttrunc)\n","  return np.round(prediction)"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"TJT_joGMLbGc","executionInfo":{"status":"ok","timestamp":1618848383054,"user_tz":-480,"elapsed":787,"user":{"displayName":"Jun Yi Chan","photoUrl":"","userId":"11999584248492493983"}}},"source":["def Tweets_Analysis(dataframe):\n","  Processed_data, x_test, word_index = Preprocess(dataframe)\n","  model = keras.models.load_model('/content/drive/My Drive/COMP3359 Project/model.h5') #change the file path\n","  predicts = model.predict(x_test) \n","  Processed_data['predict'] = np.round(predicts)\n","  return  Processed_data"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"E-Qm5xSJNbhp","executionInfo":{"status":"ok","timestamp":1618849908883,"user_tz":-480,"elapsed":564182,"user":{"displayName":"Jun Yi Chan","photoUrl":"","userId":"11999584248492493983"}}},"source":["ans=Tweets_Analysis(data)  # input the data frame here\n","\n"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":419},"id":"_PB3fDWk_FGm","executionInfo":{"status":"ok","timestamp":1618850120596,"user_tz":-480,"elapsed":642,"user":{"displayName":"Jun Yi Chan","photoUrl":"","userId":"11999584248492493983"}},"outputId":"1233d8df-02b4-456e-99a8-9677435e6007"},"source":["Processed_data['predict'] = np.round(prediction)\n","Processed_data"],"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>target</th>\n","      <th>tweet</th>\n","      <th>predict</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>[aww, that, bummer, shoulda, got, david, carr,...</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>[upset, cant, updat, facebook, text, might, cr...</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>[dive, mani, time, ball, manag, save, rest, go...</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>[whole, bodi, feel, itchi, like, fire]</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>[behav, im, mad, cant, see]</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1599995</th>\n","      <td>4</td>\n","      <td>[woke, school, best, feel, ever]</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>1599996</th>\n","      <td>4</td>\n","      <td>[thewdbcom, cool, hear, old, walt, interview, ...</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>1599997</th>\n","      <td>4</td>\n","      <td>[readi, mojo, makeov, ask, detail]</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>1599998</th>\n","      <td>4</td>\n","      <td>[happi, birthday, boo, time, tupac, amaru, sha...</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>1599999</th>\n","      <td>4</td>\n","      <td>[happi, charitytuesday]</td>\n","      <td>1.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1592682 rows × 3 columns</p>\n","</div>"],"text/plain":["         target                                              tweet  predict\n","0             0  [aww, that, bummer, shoulda, got, david, carr,...      0.0\n","1             0  [upset, cant, updat, facebook, text, might, cr...      0.0\n","2             0  [dive, mani, time, ball, manag, save, rest, go...      0.0\n","3             0             [whole, bodi, feel, itchi, like, fire]      0.0\n","4             0                        [behav, im, mad, cant, see]      0.0\n","...         ...                                                ...      ...\n","1599995       4                   [woke, school, best, feel, ever]      1.0\n","1599996       4  [thewdbcom, cool, hear, old, walt, interview, ...      1.0\n","1599997       4                 [readi, mojo, makeov, ask, detail]      1.0\n","1599998       4  [happi, birthday, boo, time, tupac, amaru, sha...      1.0\n","1599999       4                            [happi, charitytuesday]      1.0\n","\n","[1592682 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"zQPkhO3Xe55H","executionInfo":{"status":"ok","timestamp":1618844095574,"user_tz":-480,"elapsed":820,"user":{"displayName":"Jun Yi Chan","photoUrl":"","userId":"11999584248492493983"}},"outputId":"15dc0be4-e93d-492e-8f74-2d399ddef344"},"source":["ans.head(50)"],"execution_count":46,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>target</th>\n","      <th>tweet</th>\n","      <th>predict</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>[aww, that, bummer, shoulda, got, david, carr,...</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>[upset, cant, updat, facebook, text, might, cr...</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>[dive, mani, time, ball, manag, save, rest, go...</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>[whole, bodi, feel, itchi, like, fire]</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>[behav, im, mad, cant, see]</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>0</td>\n","      <td>[whole, crew]</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>0</td>\n","      <td>[need, hug]</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>0</td>\n","      <td>[hey, long, time, see, ye, rain, bit, bit, lol...</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>0</td>\n","      <td>[nope, didnt]</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>0</td>\n","      <td>[que, muera]</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>0</td>\n","      <td>[spring, break, plain, citi, snow]</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>0</td>\n","      <td>[repierc, ear]</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>0</td>\n","      <td>[couldnt, bear, watch, thought, ua, loss, emba...</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>0</td>\n","      <td>[count, idk, either, never, talk, anymor]</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>0</td>\n","      <td>[wouldv, first, didnt, gun, realli, though, za...</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>0</td>\n","      <td>[wish, got, watch, miss, premier]</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>0</td>\n","      <td>[holli, death, scene, hurt, sever, watch, film...</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>0</td>\n","      <td>[file, tax]</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>0</td>\n","      <td>[ahh, ive, alway, want, see, rent, love, sound...</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>0</td>\n","      <td>[oh, dear, drink, forgotten, tabl, drink]</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>0</td>\n","      <td>[day, didnt, get, much, done]</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>0</td>\n","      <td>[one, friend, call, ask, meet, mid, valley, to...</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>0</td>\n","      <td>[bake, cake, ate]</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>0</td>\n","      <td>[week, go, hope]</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>0</td>\n","      <td>[blagh, class, tomorrow]</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>0</td>\n","      <td>[hate, call, wake, peopl]</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>0</td>\n","      <td>[go, cri, sleep, watch, marley]</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>0</td>\n","      <td>[im, sad, misslilli]</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>0</td>\n","      <td>[ooh, lol, lesli, ok, wont, lesli, wont, get, ...</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>0</td>\n","      <td>[meh, almost, lover, except, track, get, depre...</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>30</th>\n","      <td>0</td>\n","      <td>[hack, account, aim, make, new, one]</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>31</th>\n","      <td>0</td>\n","      <td>[want, go, promot, gear, groov, unforn, ride, ...</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>32</th>\n","      <td>0</td>\n","      <td>[thought, sleep, option, tomorrow, realiz, eva...</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>33</th>\n","      <td>0</td>\n","      <td>[awe, love, miss]</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>34</th>\n","      <td>0</td>\n","      <td>[cri, asian, eye, sleep, night]</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>35</th>\n","      <td>0</td>\n","      <td>[ok, im, sick, spent, hour, sit, shower, caus,...</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>36</th>\n","      <td>0</td>\n","      <td>[ill, tell, ya, stori, later, good, day, ill, ...</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>37</th>\n","      <td>0</td>\n","      <td>[sorri, bed, time, came]</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>38</th>\n","      <td>0</td>\n","      <td>[dont, either, depress, dont, think, even, wan...</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>39</th>\n","      <td>0</td>\n","      <td>[bed, class, work, gym, class, anoth, day, tha...</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>40</th>\n","      <td>0</td>\n","      <td>[realli, dont, feel, like, get, today, got, st...</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>41</th>\n","      <td>0</td>\n","      <td>[he, reason, teardrop, guitar, one, enough, br...</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>42</th>\n","      <td>0</td>\n","      <td>[sad, sad, sad, dont, know, hate, feel, wanna,...</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>43</th>\n","      <td>0</td>\n","      <td>[aww, soo, wish, see, final, comfort, im, sad,...</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>44</th>\n","      <td>0</td>\n","      <td>[fall, asleep, heard, traci, girl, bodi, found...</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>45</th>\n","      <td>0</td>\n","      <td>[yay, im, happi, job, also, mean, less, time]</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>46</th>\n","      <td>0</td>\n","      <td>[check, user, timelin, blackberri, look, like,...</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>47</th>\n","      <td>0</td>\n","      <td>[oh, manwa, iron, fave, top, wear, meet, burnt]</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>48</th>\n","      <td>0</td>\n","      <td>[strang, sad, lilo, samro, break]</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>49</th>\n","      <td>0</td>\n","      <td>[oh, im, sorri, didnt, think, retweet]</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    target                                              tweet  predict\n","0        0  [aww, that, bummer, shoulda, got, david, carr,...      0.0\n","1        0  [upset, cant, updat, facebook, text, might, cr...      0.0\n","2        0  [dive, mani, time, ball, manag, save, rest, go...      0.0\n","3        0             [whole, bodi, feel, itchi, like, fire]      0.0\n","4        0                        [behav, im, mad, cant, see]      0.0\n","5        0                                      [whole, crew]      1.0\n","6        0                                        [need, hug]      0.0\n","7        0  [hey, long, time, see, ye, rain, bit, bit, lol...      0.0\n","8        0                                      [nope, didnt]      0.0\n","9        0                                       [que, muera]      0.0\n","10       0                 [spring, break, plain, citi, snow]      1.0\n","11       0                                     [repierc, ear]      1.0\n","12       0  [couldnt, bear, watch, thought, ua, loss, emba...      0.0\n","13       0          [count, idk, either, never, talk, anymor]      0.0\n","14       0  [wouldv, first, didnt, gun, realli, though, za...      0.0\n","15       0                  [wish, got, watch, miss, premier]      0.0\n","16       0  [holli, death, scene, hurt, sever, watch, film...      0.0\n","17       0                                        [file, tax]      0.0\n","18       0  [ahh, ive, alway, want, see, rent, love, sound...      0.0\n","19       0          [oh, dear, drink, forgotten, tabl, drink]      0.0\n","20       0                      [day, didnt, get, much, done]      0.0\n","21       0  [one, friend, call, ask, meet, mid, valley, to...      0.0\n","22       0                                  [bake, cake, ate]      1.0\n","23       0                                   [week, go, hope]      0.0\n","24       0                           [blagh, class, tomorrow]      0.0\n","25       0                          [hate, call, wake, peopl]      0.0\n","26       0                    [go, cri, sleep, watch, marley]      0.0\n","27       0                               [im, sad, misslilli]      0.0\n","28       0  [ooh, lol, lesli, ok, wont, lesli, wont, get, ...      0.0\n","29       0  [meh, almost, lover, except, track, get, depre...      0.0\n","30       0               [hack, account, aim, make, new, one]      0.0\n","31       0  [want, go, promot, gear, groov, unforn, ride, ...      0.0\n","32       0  [thought, sleep, option, tomorrow, realiz, eva...      0.0\n","33       0                                  [awe, love, miss]      1.0\n","34       0                    [cri, asian, eye, sleep, night]      0.0\n","35       0  [ok, im, sick, spent, hour, sit, shower, caus,...      0.0\n","36       0  [ill, tell, ya, stori, later, good, day, ill, ...      1.0\n","37       0                           [sorri, bed, time, came]      0.0\n","38       0  [dont, either, depress, dont, think, even, wan...      0.0\n","39       0  [bed, class, work, gym, class, anoth, day, tha...      0.0\n","40       0  [realli, dont, feel, like, get, today, got, st...      0.0\n","41       0  [he, reason, teardrop, guitar, one, enough, br...      0.0\n","42       0  [sad, sad, sad, dont, know, hate, feel, wanna,...      0.0\n","43       0  [aww, soo, wish, see, final, comfort, im, sad,...      0.0\n","44       0  [fall, asleep, heard, traci, girl, bodi, found...      0.0\n","45       0      [yay, im, happi, job, also, mean, less, time]      0.0\n","46       0  [check, user, timelin, blackberri, look, like,...      0.0\n","47       0    [oh, manwa, iron, fave, top, wear, meet, burnt]      0.0\n","48       0                  [strang, sad, lilo, samro, break]      0.0\n","49       0             [oh, im, sorri, didnt, think, retweet]      0.0"]},"metadata":{"tags":[]},"execution_count":46}]}]}